{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install yt-dlp moviepy pydub nltk rake-nltk scikit-learn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt update\n",
    "# sudo apt install ffmpeg\n",
    "# ffmpeg -version\n",
    "# ffprobe -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=X8MZWCGgIb8\n",
      "[youtube] X8MZWCGgIb8: Downloading webpage\n",
      "[youtube] X8MZWCGgIb8: Downloading ios player API JSON\n",
      "[youtube] X8MZWCGgIb8: Downloading web creator player API JSON\n",
      "[youtube] X8MZWCGgIb8: Downloading m3u8 information\n",
      "[info] X8MZWCGgIb8: Downloading 1 format(s): 251\n",
      "[download] Destination: audio.webm\n",
      "[download] 100% of    5.32MiB in 00:00:00 at 10.39MiB/s  \n",
      "[ExtractAudio] Destination: audio.wav\n",
      "Deleting original file audio.webm (pass -k to keep)\n",
      "MoviePy - Writing audio in audio_output.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp as youtube_dl\n",
    "from moviepy.editor import AudioFileClip\n",
    "\n",
    "# Download YouTube video\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': 'audio.%(ext)s',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "url = \"https://www.youtube.com/watch?v=X8MZWCGgIb8\"\n",
    "\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n",
    "\n",
    "# Extract Audio\n",
    "audio_path = 'audio_output.wav'\n",
    "audio_clip = AudioFileClip(\"audio.wav\")\n",
    "audio_clip.write_audiofile(audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import speech_recognition as sr\n",
    "\n",
    "def recognize_speech_with_retry(audio_file, retries=3, delay=5):\n",
    "    recognizer = sr.Recognizer()\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            with sr.AudioFile(audio_file) as source:\n",
    "                audio = recognizer.record(source)\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                return text\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: two computer engineers and best friends to decide to play the game of flip the coin on tapes computer with the computer being the opponent and the computer plays the first move but it doesn't know what it was not true what is the quantum computer and quantum computers are advanced machines inspired by Quantum Physics study of the behaviour of atoms and particles so quantum computers operate by studying in controlling behaviour of these particles within away completely different computers or even supercomputers it is an upgraded and not exactly the next generation questions because whether you choose to flip the coin or not the outcome of still be there between both possibilities just like a mixture of lemon juice and water lemon juice is very little messages from one location to another it can be difficult for people without condom and certainly this type of unique and Unbreakable corruption is already tested by banks and companies like JP morgan's etc can be used in the medicinal world computer it is connected to over quantum computers are the future of upcoming generation work it is still taking over the technology in the business world companies like Google Microsoft or competing to build Quantum computing tools this progress some of the benefits of quantum computation to be realised long before the search for large\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def split_audio(file_path, chunk_length_ms=60000):\n",
    "    audio = AudioSegment.from_wav(file_path)\n",
    "    chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "    chunk_files = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = f\"chunk_{i}.wav\"\n",
    "        chunk.export(chunk_filename, format=\"wav\")\n",
    "        chunk_files.append(chunk_filename)\n",
    "    \n",
    "    return chunk_files\n",
    "\n",
    "# Split the audio into chunks\n",
    "chunks = split_audio(\"audio_output.wav\")\n",
    "\n",
    "# Process each chunk individually\n",
    "all_text = []\n",
    "for chunk in chunks:\n",
    "    text = recognize_speech_with_retry(chunk)\n",
    "    if text:\n",
    "        all_text.append(text)\n",
    "    os.remove(chunk)  # Clean up the chunk file after processing\n",
    "\n",
    "final_text = \" \".join(all_text)\n",
    "print(\"Extracted Text:\", final_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: quantum computers future upcoming generation work still taking technology business world companies like google microsoft competing build quantum computing tools progress benefits quantum computation realised long search large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jarin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jarin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenization\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stopwords]  # Remove stopwords\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "processed_text = preprocess_text(text)\n",
    "print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['quantum computers future upcoming generation work still taking technology business world companies like google microsoft competing build quantum computing tools progress benefits quantum computation realised long search large']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "# Initialize RAKE\n",
    "r = Rake()\n",
    "\n",
    "# Extract keywords from the text\n",
    "r.extract_keywords_from_text(processed_text)\n",
    "\n",
    "# Get ranked phrases\n",
    "keywords = r.get_ranked_phrases()\n",
    "\n",
    "print(\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords with Scores: [('quantum', 0.5222329678670935), ('benefits', 0.17407765595569785), ('build', 0.17407765595569785), ('business', 0.17407765595569785), ('companies', 0.17407765595569785), ('competing', 0.17407765595569785), ('computation', 0.17407765595569785), ('computers', 0.17407765595569785), ('computing', 0.17407765595569785), ('future', 0.17407765595569785), ('generation', 0.17407765595569785), ('google', 0.17407765595569785), ('large', 0.17407765595569785), ('like', 0.17407765595569785), ('long', 0.17407765595569785), ('microsoft', 0.17407765595569785), ('progress', 0.17407765595569785), ('realised', 0.17407765595569785), ('search', 0.17407765595569785), ('taking', 0.17407765595569785), ('technology', 0.17407765595569785), ('tools', 0.17407765595569785), ('upcoming', 0.17407765595569785), ('work', 0.17407765595569785), ('world', 0.17407765595569785)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [processed_text]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names and scores\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "scores = X.T.sum(axis=1).A1\n",
    "\n",
    "# Create a dictionary of keywords and their scores\n",
    "keywords = dict(zip(feature_names, scores))\n",
    "\n",
    "# Sort and print keywords\n",
    "sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Keywords with Scores:\", sorted_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: comp.graphics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Example: Using 20 Newsgroups Dataset for Classification\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "\n",
    "# Train a simple model\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline([('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])\n",
    "model.fit(X_train, y_train)\n",
    "predicted_category = model.predict([processed_text])\n",
    "\n",
    "print(\"Predicted Category:\", newsgroups.target_names[predicted_category[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities: [0.00235485 0.         0.01736099 ... 0.01278517 0.00950991 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vectorize Text\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([processed_text] + newsgroups.data)\n",
    "\n",
    "# Cosine Similarity\n",
    "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "similarities = cos_sim[0][1:]  # Exclude the first comparison with itself\n",
    "\n",
    "print(\"Cosine Similarities:\", similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Videos:\n",
      "Similarity: 0.08310863269725427\n",
      "Doc Text: From: danj@welchgate.welch.jhu.edu (Dan Jacobson)\n",
      "Subject: Re: Is there an FTP achive for USGS terra...\n",
      "\n",
      "Similarity: 0.07108841793384753\n",
      "Doc Text: From: peterbak@microsoft.com (Peter Bako)\n",
      "Subject: JPEG file format?\n",
      "Organization: Microsoft Corp.\n",
      "L...\n",
      "\n",
      "Similarity: 0.06337028515567467\n",
      "Doc Text: From: mangoe@cs.umd.edu (Charley Wingate)\n",
      "Subject: Re: Yeah, Right\n",
      "Lines: 30\n",
      "\n",
      "Benedikt Rosenau write...\n",
      "\n",
      "Similarity: 0.05237382670904888\n",
      "Doc Text: From: ingles@engin.umich.edu (Ray Ingles)\n",
      "Subject: Re: Yeah, Right\n",
      "Organization: University of Michi...\n",
      "\n",
      "Similarity: 0.051638653385994784\n",
      "Doc Text: From: jenk@microsoft.com (Jen Kilmer)\n",
      "Subject: Re: sex education\n",
      "Organization: Microsoft Corporation...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Recommend Docs Based on Similarity\n",
    "top_indices = np.argsort(similarities)[-5:][::-1]  # Get top 5 most similar\n",
    "recommended_videos = [(newsgroups.data[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "print(\"Recommended Videos:\")\n",
    "for video, sim in recommended_videos:\n",
    "    print(f\"Similarity: {sim}\\nDoc Text: {video[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
